---
output:
  html_document: default
  pdf_document: default
---

# Data Mining Lab 3 - Classification

---
Title: Data Mining Lab 3
Authors: Max Link, Logan Lu, Jadon Klipsch
Date: "2025-04-14"
Description: In this project, we will focus on classification 
---

```{r setup, include=FALSE}
# Disable RStudio's package update check for this session
options("rstudio.package_manager.check_updates" = FALSE)

# Set the CRAN repository
options(repos = c(CRAN = "https://cloud.r-project.org"))

# Install packages quietly
if (!requireNamespace("xfun", quietly = TRUE)) {
  install.packages("xfun", quietly = TRUE)
}
if (!requireNamespace("ggrepel", quietly = TRUE)) {
  install.packages("ggrepel", quietly = TRUE)
}

# Set knitr options
knitr::opts_chunk$set(echo = TRUE)

# Load required libraries and suppress startup messages
suppressPackageStartupMessages({
  library(dplyr)      # For data manipulation
  library(ggplot2)    # For visualizations
  library(tidyr)      # For cleaning data
  library(ggrepel)    # For repelling labels in plots
})
```

```{r, results='hide', message=FALSE, warning=FALSE}
# add packages to install here
pkgs <- c("tidyverse", "factoextra", "cluster", "patchwork", "tibble", "ggrepel",
          "mclust", "mcclust", "fpc", "seriation", "apcluster", "dbscan", "entropy", "maps", "kernlab", "skimr", "caret", "randomForest", "pROC", "cli")
pkgs_install <- pkgs[!(pkgs %in% installed.packages()[,"Package"])]
if(length(pkgs_install)) install.packages(pkgs_install)

suppressPackageStartupMessages(library(tidyverse))
suppressPackageStartupMessages(library(readr))
suppressPackageStartupMessages(library(dplyr))
suppressPackageStartupMessages(library(tidyr))
suppressPackageStartupMessages(library(knitr))
suppressPackageStartupMessages(library(tibble))
suppressPackageStartupMessages(library(mclust))
suppressPackageStartupMessages(library(mcclust))
suppressPackageStartupMessages(library(ggplot2))
suppressPackageStartupMessages(library(ggrepel))
suppressPackageStartupMessages(library(cluster))
suppressPackageStartupMessages(library(patchwork))
suppressPackageStartupMessages(library(factoextra))
suppressPackageStartupMessages(library(fpc))
suppressPackageStartupMessages(library(seriation))
suppressPackageStartupMessages(library(apcluster))
suppressPackageStartupMessages(library(dbscan))
suppressPackageStartupMessages(library(entropy))
suppressPackageStartupMessages(library(maps))
suppressPackageStartupMessages(library(skimr))
suppressPackageStartupMessages(library(caret))
suppressPackageStartupMessages(library(randomForest))
suppressPackageStartupMessages(library(pROC))
suppressPackageStartupMessages(library(cli))

``` 

```{r, results='hide', message=FALSE, warning=FALSE}


# Load libraries
library(tidyverse)
library(readr)
library(dplyr)
library(tidyr)
library(knitr)
library(tibble)
library(mclust)
library(mcclust)
library(ggplot2)
library(ggrepel)
library(cluster)
library(patchwork)
library(factoextra)
library(fpc)
library(seriation)
library(apcluster)
library(dbscan)
library(entropy)
library(maps)
library(skimr)


```

```{r}

dataset <- read.csv("tx_data_with_gr.csv")

# print(dataset)
```

### Section 1 -- Data Prep

First, we need to define our classes.

We can pick out two classes: 

1) High vs Low Infection Rate 



2) High vs Low Death Rate 



Now we can explore statistics to get proper thresholds

```{r}
# 1) Explore distribution of infection_rate and death_rate
# Summary statistics
cat("Summary of Infection Rate:\n")
summary(dataset$infection_rate)
cat("\nSummary of Death Rate:\n")
summary(dataset$death_rate)

```
The median is appropriate for both of these classification tasks because choosing the median balances our classes. Dealing with balanced classes means that we have a close 50-50 county split into high and low categories. This even split enables logistic regression, decision trees, SVMs. 

The infection rate threshold should be 0.07212. 

The death rate threshold should be 0.022603. 

```{r}
# Load required library (optional, for table creation)
library(dplyr)

# Read the dataset (update path as needed)
dataset <- read.csv("tx_data_with_gr.csv", stringsAsFactors = FALSE)

# Total number of counties
total_counties <- nrow(dataset)

# 1) High vs. Low Infection Rate (threshold: 0.1)
# Count counties with infection_rate > 0.1 (High)
high_infection <- sum(dataset$infection_rate > 0.07212, na.rm = TRUE)
# Count counties with infection_rate <= 0.1 (Low)
low_infection <- sum(dataset$infection_rate <= 0.07212, na.rm = TRUE)
# Calculate percentages
high_infection_pct <- (high_infection / total_counties) * 100
low_infection_pct <- (low_infection / total_counties) * 100

# 2) High vs. Low Death Rate (threshold: 0.023, updated from 0.0023)
# Count counties with death_rate > 0.023 (High)
high_death <- sum(dataset$death_rate > 0.022603, na.rm = TRUE)
# Count counties with death_rate <= 0.023 (Low)
low_death <- sum(dataset$death_rate <= 0.022603, na.rm = TRUE)
# Calculate percentages
high_death_pct <- (high_death / total_counties) * 100
low_death_pct <- (low_death / total_counties) * 100

# Print results
cat("1) Infection Rate Classification (Threshold: 0.07212, i.e., 1,000 cases per 10,000 population)\n")
cat("High Infection Rate (> 0.07212):", high_infection, "counties (", round(high_infection_pct, 2), "%)\n")
cat("Low Infection Rate (<= 0.07212):", low_infection, "counties (", round(low_infection_pct, 2), "%)\n\n")

cat("2) Death Rate Classification (Threshold: 0.022603, i.e., 230 deaths per 10,000 population)\n")
cat("High Death Rate (> 0.022603):", high_death, "counties (", round(high_death_pct, 2), "%)\n")
cat("Low Death Rate (<= 0.022603):", low_death, "counties (", round(low_death_pct, 2), "%)\n\n")

```

Now we can make our table with our class attributes for infection rate classification and death rate classification. 

```{r}
# Create class attributes for Infection Rate and Death Rate
dataset <- dataset %>%
  mutate(Infection_Rate_Class = ifelse(infection_rate > 0.07212, "High", "Low"),
         Death_Rate_Class = ifelse(death_rate > 0.022603, "High", "Low"))

# Select features for model learning, including both class attributes
table_for_model <- dataset %>%
  select(county_name, median_age, median_income, total_pop, death_rate, infection_rate,
         Infection_Rate_Class, Death_Rate_Class)

# Display first 10 rows
cat("\nSingle Table with Infection_Rate_Class and Death_Rate_Class (First 10 Rows):\n")
print(head(table_for_model, 10))

# Verify class distributions
# Infection Rate
infection_dist <- table(dataset$Infection_Rate_Class)
infection_prop <- prop.table(infection_dist)
cat("\nInfection Rate Class Distribution:\n")
print(infection_dist)
cat("\nInfection Rate Class Proportions:\n")
print(infection_prop)

# Death Rate
death_dist <- table(dataset$Death_Rate_Class)
death_prop <- prop.table(death_dist)
cat("\nDeath Rate Class Distribution:\n")
print(death_dist)
cat("\nDeath Rate Class Proportions:\n")
print(death_prop)

# Save the table
write.csv(table_for_model, "tx_data_with_class.csv", row.names = FALSE)

```

Our predictive features are: 

1) median age 

2) median income 

3) total population 


### Section 2 -- Modeling 

```{r}
# Load required libraries
library(dplyr)
library(caret)
library(randomForest)
library(pROC)

```

```{r} 
# Dataset loading and verification 

# Step 1: Verify dataset structure
cat("\nColumn names in dataset:\n")
print(names(dataset))
cat("\nNumber of counties in dataset:", nrow(dataset), "\n")
cat("\nSummary of infection_rate:\n")
print(summary(dataset$infection_rate))
cat("\nSummary of death_rate:\n")
print(summary(dataset$death_rate))

# Check if required columns exist
if (!all(c("infection_rate", "death_rate", "median_age", "median_income", "total_pop") %in% names(dataset))) {
  stop("Required columns (infection_rate, death_rate, median_age, median_income, total_pop) not found in dataset.")
}

```

```{r}
# Step 2: Data Preparation
# Create class attributes and ensure they are factors
dataset <- dataset %>%
  mutate(
    Infection_Rate_Class = factor(ifelse(infection_rate > 0.07212, "High", "Low"), levels = c("Low", "High")),
    Death_Rate_Class = factor(ifelse(death_rate > 0.022603, "High", "Low"), levels = c("Low", "High"))
  )

# Check unique values and distribution in Infection_Rate_Class
cat("\nUnique values in Infection_Rate_Class:\n")
print(unique(dataset$Infection_Rate_Class))
cat("\nClass distribution:\n")
print(table(dataset$Infection_Rate_Class))

# Select features and class attribute (focusing on Infection_Rate_Class)
# Exclude infection rate to prevent data leakage 
model_data <- dataset %>%
  select(median_age, median_income, total_pop, Infection_Rate_Class)

# Check for missing values
cat("\nMissing Values:\n")
print(colSums(is.na(model_data)))

# Handle missing values (impute with median if any)
model_data <- model_data %>%
  mutate(
    median_age = ifelse(is.na(median_age), median(median_age, na.rm = TRUE), median_age),
    median_income = ifelse(is.na(median_income), median(median_income, na.rm = TRUE), median_income),
    total_pop = ifelse(is.na(total_pop), median(total_pop, na.rm = TRUE), total_pop)
  )

# Scale numerical features (standardize to mean = 0, sd = 1)
model_data <- model_data %>%
  mutate(
    median_age = scale(median_age),
    median_income = scale(median_income),
    total_pop = scale(total_pop)
  )

# Train-test split (80% train, 20% test, stratified)
set.seed(123) # For reproducibility
train_index <- createDataPartition(model_data$Infection_Rate_Class, p = 0.8, list = FALSE)
train_data <- model_data[train_index, ]
test_data <- model_data[-train_index, ]

# Verify class distribution in train and test sets
cat("\nTraining Set Class Distribution:\n")
print(table(train_data$Infection_Rate_Class))
cat("\nTesting Set Class Distribution:\n")
print(table(test_data$Infection_Rate_Class))

```


```{r}

# Step 3: Training and Testing
# Train Random Forest model
rf_model <- randomForest(Infection_Rate_Class ~ median_age + median_income + total_pop,
                         data = train_data, ntree = 500)

# Predict on test set
predictions <- predict(rf_model, test_data)
prob_predictions <- predict(rf_model, test_data, type = "prob")[, "High"]

# Evaluate model performance
conf_matrix <- confusionMatrix(predictions, test_data$Infection_Rate_Class)
cat("\nConfusion Matrix and Metrics:\n")
print(conf_matrix)

# Calculate AUC-ROC
roc_obj <- roc(test_data$Infection_Rate_Class, prob_predictions)
cat("\nAUC-ROC:\n")
print(auc(roc_obj))


```

```{r}
# Step 4: Hyperparameter Tuning
# Define hyperparameter grid (only mtry for caret's rf method)
tune_grid <- expand.grid(mtry = c(1, 2, 3))

# Custom trainControl for cross-validation
ctrl <- trainControl(
  method = "cv",
  number = 5, # 5-fold cross-validation
  summaryFunction = twoClassSummary, # For ROC, sensitivity, specificity
  classProbs = TRUE,
  savePredictions = TRUE
)

# Tune Random Forest using caret
set.seed(123)
rf_tuned <- train(
  Infection_Rate_Class ~ median_age + median_income + total_pop,
  data = train_data,
  method = "rf",
  trControl = ctrl,
  tuneGrid = tune_grid,
  metric = "ROC",
  ntree = 500 # Set ntree directly instead of tuning
)

# Print tuning results
cat("\nHyperparameter Tuning Results:\n")
print(rf_tuned)

# Best model
best_model <- rf_tuned$finalModel

# Evaluate best model on test set
best_predictions <- predict(best_model, test_data)
best_prob_predictions <- predict(best_model, test_data, type = "prob")[, "High"]

# Confusion matrix for best model
best_conf_matrix <- confusionMatrix(best_predictions, test_data$Infection_Rate_Class)
cat("\nBest Model Confusion Matrix and Metrics:\n")
print(best_conf_matrix)

# AUC-ROC for best model
best_roc_obj <- roc(test_data$Infection_Rate_Class, best_prob_predictions)
cat("\nBest Model AUC-ROC:\n")
print(auc(best_roc_obj))

# Save the prepared data (optional, for reference)
# write.csv(model_data, "tx_data_prepared.csv", row.names = FALSE)


```