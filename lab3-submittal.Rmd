---
output:
  html_document: default
  pdf_document: default
---

# Data Mining Lab 3 - Classification

---
Title: Data Mining Lab 3
Authors: Max Link, Logan Lu, Jadon Klipsch
Date: "2025-04-14"
Description: In this project, we will focus on classification 
---

```{r setup, include=FALSE}
# Disable RStudio's package update check for this session
options("rstudio.package_manager.check_updates" = FALSE)

# Set the CRAN repository
options(repos = c(CRAN = "https://cloud.r-project.org"))

# Install packages quietly
if (!requireNamespace("xfun", quietly = TRUE)) {
  install.packages("xfun", quietly = TRUE)
}
if (!requireNamespace("ggrepel", quietly = TRUE)) {
  install.packages("ggrepel", quietly = TRUE)
}

# Set knitr options
knitr::opts_chunk$set(echo = TRUE)

# Load required libraries and suppress startup messages
suppressPackageStartupMessages({
  library(dplyr)      # For data manipulation
  library(ggplot2)    # For visualizations
  library(tidyr)      # For cleaning data
  library(ggrepel)    # For repelling labels in plots
  library(caret)
  library(pROC)
  library(dplyr)

})
```

```{r, results='hide', message=FALSE, warning=FALSE}
# add packages to install here
pkgs <- c("tidyverse", "factoextra", "cluster", "patchwork", "tibble", "ggrepel",
          "mclust", "mcclust", "fpc", "seriation", "apcluster", "dbscan", "entropy", "maps", "kernlab", "skimr", "caret", "randomForest", "pROC", "cli")
pkgs_install <- pkgs[!(pkgs %in% installed.packages()[,"Package"])]
if(length(pkgs_install)) install.packages(pkgs_install)

suppressPackageStartupMessages(library(tidyverse))
suppressPackageStartupMessages(library(readr))
suppressPackageStartupMessages(library(dplyr))
suppressPackageStartupMessages(library(tidyr))
suppressPackageStartupMessages(library(knitr))
suppressPackageStartupMessages(library(tibble))
suppressPackageStartupMessages(library(mclust))
suppressPackageStartupMessages(library(mcclust))
suppressPackageStartupMessages(library(ggplot2))
suppressPackageStartupMessages(library(ggrepel))
suppressPackageStartupMessages(library(cluster))
suppressPackageStartupMessages(library(patchwork))
suppressPackageStartupMessages(library(factoextra))
suppressPackageStartupMessages(library(fpc))
suppressPackageStartupMessages(library(seriation))
suppressPackageStartupMessages(library(apcluster))
suppressPackageStartupMessages(library(dbscan))
suppressPackageStartupMessages(library(entropy))
suppressPackageStartupMessages(library(maps))
suppressPackageStartupMessages(library(skimr))
suppressPackageStartupMessages(library(caret))
suppressPackageStartupMessages(library(randomForest))
suppressPackageStartupMessages(library(pROC))
suppressPackageStartupMessages(library(cli))

``` 

```{r, results='hide', message=FALSE, warning=FALSE}


# Load libraries
library(tidyverse)
library(readr)
library(dplyr)
library(tidyr)
library(knitr)
library(tibble)
library(mclust)
library(mcclust)
library(ggplot2)
library(ggrepel)
library(cluster)
library(patchwork)
library(factoextra)
library(fpc)
library(seriation)
library(apcluster)
library(dbscan)
library(entropy)
library(maps)
library(skimr)


```

```{r}

dataset <- read.csv("tx_data_with_gr.csv")

# print(dataset)
```

### Section 1 -- Data Prep

First, we need to define our classes.

We can pick out two classes: 

1) High vs Low Infection Rate 



2) High vs Low Death Rate 



Now we can explore statistics to get proper thresholds

```{r}
# 1) Explore distribution of infection_rate and death_rate
# Summary statistics
cat("Summary of Infection Rate:\n")
summary(dataset$infection_rate)
cat("\nSummary of Death Rate:\n")
summary(dataset$death_rate)

```
The median is appropriate for both of these classification tasks because choosing the median balances our classes. Dealing with balanced classes means that we have a close 50-50 county split into high and low categories. This even split enables logistic regression, decision trees, SVMs. 

The infection rate threshold should be 0.07212. 

The death rate threshold should be 0.022603. 

```{r}
# Load required library (optional, for table creation)
library(dplyr)

# Read the dataset (update path as needed)
dataset <- read.csv("tx_data_with_gr.csv", stringsAsFactors = FALSE)

# Total number of counties
total_counties <- nrow(dataset)

# 1) High vs. Low Infection Rate (threshold: 0.1)
# Count counties with infection_rate > 0.1 (High)
high_infection <- sum(dataset$infection_rate > 0.07212, na.rm = TRUE)
# Count counties with infection_rate <= 0.1 (Low)
low_infection <- sum(dataset$infection_rate <= 0.07212, na.rm = TRUE)
# Calculate percentages
high_infection_pct <- (high_infection / total_counties) * 100
low_infection_pct <- (low_infection / total_counties) * 100

# 2) High vs. Low Death Rate (threshold: 0.023, updated from 0.0023)
# Count counties with death_rate > 0.023 (High)
high_death <- sum(dataset$death_rate > 0.022603, na.rm = TRUE)
# Count counties with death_rate <= 0.023 (Low)
low_death <- sum(dataset$death_rate <= 0.022603, na.rm = TRUE)
# Calculate percentages
high_death_pct <- (high_death / total_counties) * 100
low_death_pct <- (low_death / total_counties) * 100

# Print results
cat("1) Infection Rate Classification (Threshold: 0.07212, i.e., 1,000 cases per 10,000 population)\n")
cat("High Infection Rate (> 0.07212):", high_infection, "counties (", round(high_infection_pct, 2), "%)\n")
cat("Low Infection Rate (<= 0.07212):", low_infection, "counties (", round(low_infection_pct, 2), "%)\n\n")

cat("2) Death Rate Classification (Threshold: 0.022603, i.e., 230 deaths per 10,000 population)\n")
cat("High Death Rate (> 0.022603):", high_death, "counties (", round(high_death_pct, 2), "%)\n")
cat("Low Death Rate (<= 0.022603):", low_death, "counties (", round(low_death_pct, 2), "%)\n\n")

```

Now we can make our table with our class attributes for infection rate classification and death rate classification. 

```{r}
# Create class attributes for Infection Rate and Death Rate
dataset <- dataset %>%
  mutate(Infection_Rate_Class = ifelse(infection_rate > 0.07212, "High", "Low"),
         Death_Rate_Class = ifelse(death_rate > 0.022603, "High", "Low"))

# Select features for model learning, including both class attributes
table_for_model <- dataset %>%
  select(county_name, median_age, median_income, total_pop, death_rate, infection_rate,
         Infection_Rate_Class, Death_Rate_Class)

# Display first 10 rows
cat("\nSingle Table with Infection_Rate_Class and Death_Rate_Class (First 10 Rows):\n")
print(head(table_for_model, 10))

# Verify class distributions
# Infection Rate
infection_dist <- table(dataset$Infection_Rate_Class)
infection_prop <- prop.table(infection_dist)
cat("\nInfection Rate Class Distribution:\n")
print(infection_dist)
cat("\nInfection Rate Class Proportions:\n")
print(infection_prop)

# Death Rate
death_dist <- table(dataset$Death_Rate_Class)
death_prop <- prop.table(death_dist)
cat("\nDeath Rate Class Distribution:\n")
print(death_dist)
cat("\nDeath Rate Class Proportions:\n")
print(death_prop)

# Save the table
write.csv(table_for_model, "tx_data_with_class.csv", row.names = FALSE)

```

Our predictive features are: 

1) median age 

2) median income 

3) total population 


### Section 2 -- Modeling 

```{r}
# Load required libraries
library(dplyr)
library(caret)
library(randomForest)
library(pROC)

```

```{r} 
# Dataset loading and verification 

# Check if required columns exist
if (!all(c("infection_rate", "death_rate", "median_age", "median_income", "total_pop") %in% names(dataset))) {
  stop("Required columns (infection_rate, death_rate, median_age, median_income, total_pop) not found in dataset.")
}

```

```{r}
# Step 2: Data Preparation
# Create class attributes and ensure they are factors
dataset <- dataset %>%
  mutate(
    Infection_Rate_Class = factor(ifelse(infection_rate > 0.07212, "High", "Low"), levels = c("Low", "High")),
    Death_Rate_Class = factor(ifelse(death_rate > 0.022603, "High", "Low"), levels = c("Low", "High"))
  )

# Check class distribution
cat("Class distribution for Infection_Rate_Class:\n")
print(table(dataset$Infection_Rate_Class))

# Select features and class attribute
model_data <- dataset %>%
  select(median_age, median_income, total_pop, Infection_Rate_Class)

# Check for missing values
cat("\nMissing Values:\n")
print(colSums(is.na(model_data)))

# Handle missing values (impute with median if any)
model_data <- model_data %>%
  mutate(
    median_age = ifelse(is.na(median_age), median(median_age, na.rm = TRUE), median_age),
    median_income = ifelse(is.na(median_income), median(median_income, na.rm = TRUE), median_income),
    total_pop = ifelse(is.na(total_pop), median(total_pop, na.rm = TRUE), total_pop)
  )

# Scale numerical features
model_data <- model_data %>%
  mutate(
    median_age = scale(median_age),
    median_income = scale(median_income),
    total_pop = scale(total_pop)
  )

# Train-test split (80% train, 20% test, stratified)
set.seed(123)
train_index <- createDataPartition(model_data$Infection_Rate_Class, p = 0.8, list = FALSE)
train_data <- model_data[train_index, ]
test_data <- model_data[-train_index, ]

# Verify class distribution in train and test sets
cat("Training Set Class Distribution:\n")
print(table(train_data$Infection_Rate_Class))
cat("Testing Set Class Distribution:\n")
print(table(test_data$Infection_Rate_Class))

```

We can use Random Forest as our model because we want to classify 213 counties, which is a small sample size. Random Forest is appropriate for its ability to handle non-linear relationships, and prevent over-fitting.


```{r}

# Step 3: Train and Assess Random Forest
# Define train Control for cross-validation
ctrl <- trainControl(
  method = "cv",
  number = 5, # number of cross validation folds 
  summaryFunction = twoClassSummary,
  classProbs = TRUE,
  savePredictions = TRUE
) 

# Train Random Forest with tuning
set.seed(123)
rf_tuned <- train(
  Infection_Rate_Class ~ median_age + median_income + total_pop,
  data = train_data,
  method = "rf",
  trControl = ctrl,
  tuneGrid = expand.grid(mtry = c(1, 2, 3)),
  metric = "ROC",
  ntree = 500 # use 500 trees 
)

# Extract best model
best_model <- rf_tuned$finalModel

# Cross-Validation Performance
cat("\nRandom Forest Cross-Validation Results:\n")
print(rf_tuned$results)

# Test Set Performance
rf_test_predictions <- predict(best_model, test_data)
rf_test_prob_predictions <- predict(best_model, test_data, type = "prob")[, "High"]
rf_test_conf_matrix <- confusionMatrix(rf_test_predictions, test_data$Infection_Rate_Class)
rf_test_roc_obj <- roc(test_data$Infection_Rate_Class, rf_test_prob_predictions)
cat("\nRandom Forest Test Set Confusion Matrix and Metrics:\n")
print(rf_test_conf_matrix)
cat("Random Forest Test Set AUC-ROC:\n")
print(auc(rf_test_roc_obj))

# Training Set Performance
rf_train_predictions <- predict(best_model, train_data)
rf_train_conf_matrix <- confusionMatrix(rf_train_predictions, train_data$Infection_Rate_Class)
cat("\nRandom Forest Training Set Confusion Matrix and Metrics:\n")
print(rf_train_conf_matrix)


```

Our Accuracy is significantly better than random guessing with 71%, so Random Forest does have predictive power. However, Random Forest misclassifies 6 low counties as high counties and 6 high counties as low counties, which is a 28.57% error rate per class (6/21). This is substantial for a small test set of only 42 samples. In a public health context, this could have severe consequences like misallocating resources. 

Looking at the training Accuracy, we see that Random Forest has classified the training set perfectly with an accuracy of 1. This means that Random Forest has fully memorized the training set. This makes sense because random forest has extreme flexibility and the data has a small feature set (only 3 features), however the accuracy drop off of 71% for the testing set suggests severe data overfitting. This demonstrates Random Forest's ability to fit data, but raises concerns about overfitting data. This suggests that Random Forest may be too complex for this dataset and that simpler models like linear regression may perform better. 

This is backed up by the Kappa, which reads only 42%. This means that the model is only moderately above classifying by chance. 

A 95% Confidence Interval (CI) is wide, which means there is uncertainty about the test set. 

The p-value of 1.0 means there is no significant difference between errors in test sets, reflecting the balanced misclassification of 6 counties per test set. 

We have cross validation performance and training set performance with a confusion matrix/metrics. Looking at the cross validation results, we see the Mtry values ranging from 1 to 3. Mtry specifies the number of features that are used to determine the best split at a node. We have three predictors, so Mtry can be 1 (median age), 2 (median age, median income), or 3 (median age, median income, total population).


Mtry equal to 2 has the highest ROC score. Having an ROC (ability to discriminate between high and low classes) around 0.67 is moderate. An ROC of 0.67 is moderate because it is below 0.70, which means that total population, median age, and median income are not strong predictors of infection rate. 

The sens (sensitivity) reading identifies the positive class, which in this case is low. About 55% is low which means the random forest model struggles to identify low infection rate counties. 

Specificity is the proportion of high cases correctly identified. About 66% is moderate, which means our random forest model misses many high case counties in its classification. 

The standard deviations (ROCSD, SensSD, SpecSD) show variability across Cross validation folds. mtry 2 has ROCD of 0.0656 which suggests moderate variability in performance. This is most likely from our small dataset of only 171 counties in our training set. 

We can also get the feature importance from the model 

```{r}
# Extract feature importance from rf_tuned using caret's varImp()
importance <- varImp(rf_tuned)

# Display the importance scores
cat("\nRandom Forest Feature Importance:\n")
print(importance)

# Optionally, plot the feature importance for visualization
plot(importance, main = "Feature Importance for Random Forest")

```
This tells us that median_age is the most important feature for predeicting infection rate out of our three predictive features. 

Random Forest may not capture linear relationships as well as linear regression or interactions as well as SVM, so now we can try those. 

```{r}
# Model 2: Logistic Regression
lr_model <- train(
  Infection_Rate_Class ~ median_age + median_income + total_pop,
  data = train_data,
  method = "glm",
  family = "binomial",
  trControl = ctrl,
  metric = "ROC"
)
lr_predictions <- predict(lr_model, test_data)
lr_prob_predictions <- predict(lr_model, test_data, type = "prob")[, "High"]
lr_conf_matrix <- confusionMatrix(lr_predictions, test_data$Infection_Rate_Class)
lr_roc_obj <- roc(test_data$Infection_Rate_Class, lr_prob_predictions)
cat("\nLogistic Regression Confusion Matrix and Metrics:\n")
print(lr_conf_matrix)
cat("\nLogistic Regression AUC-ROC:\n")
print(auc(lr_roc_obj))
cat("\nLogistic Regression CV Results:\n")
print(lr_model$results)

# Logistic Regression Training Performance
lr_train_predictions <- predict(lr_model, train_data)
lr_train_conf_matrix <- confusionMatrix(lr_train_predictions, train_data$Infection_Rate_Class)
cat("\nLogistic Regression Training Confusion Matrix:\n")
print(lr_train_conf_matrix)

```

We have a 75% ROC, which indicates a moderate to good ability to distinguish between high and low infection rates. 

The sensitivity is better than Random Forest's, coming in at 65%. Specificity is also better than random forests coming in at 71%. This improves performance in identifying high infection rate counties. 

The standard deviations (ROCD, SensSD, SpecSD) is lower than random forests, which indicates less variability between the 5 cross folds. 

The accuracy is 66% which is lower than Random Forests accuracy. This means that logistic regression generalizes less effectively in this case. 

8 low counties were misclassified as high. 6 high counties were misclassified as low. The error rate per class was 38% for the low class (8/21) and 28% for the high class (6/21). 

Logistic regression performs worse for the low class, but matches random forest for the high class. 

Logistic regression has worse generalization than random forest, but has less over-fitting with a close alignment between its training (0.7135) and test accuracies (0.6667). 


```{r}
# Extract and display the coefficients for the Logistic Regression model
cat("\nLogistic Regression Coefficients:\n")
print(summary(lr_model$finalModel))
```
The intercept is very close to 0 (-0.0061), meaning that when all features are at their mean, the log-odds of being Low vs. High are nearly balanced (close to 0, corresponding to a probability of ~50% for Low). However, the p-value (0.972) indicates this is not significant, so we won’t focus on the intercept for practical insights


The p-value (1.47e-07) is highly significant (***), indicating strong evidence that median_age affects the infection rate classification


Since a decrease in the odds of Low increases the odds of High, the odds of High increase by a factor of 1/0.312 = 3.205, or 220.5% (3.205 - 1)
 
Counties with older populations (higher median_age) are more likely to have High infection rates. For every standard deviation increase in median_age, the odds of a county being classified as High infection rate increase by 220.5%. This suggests that age is a critical risk factor, possibly because older populations are more vulnerable to severe outcomes or have higher transmission rates due to social or healthcare factors

All the other attributes have a low p value so they are not statistically significant. 



```{r}

# Model 3: SVM with Radial Kernel
svm_model <- train(
  Infection_Rate_Class ~ median_age + median_income + total_pop,
  data = train_data,
  method = "svmRadial",
  trControl = ctrl,
  tuneGrid = expand.grid(sigma = c(0.1, 0.5, 1), C = c(0.1, 1, 10)),
  metric = "ROC"
)
svm_predictions <- predict(svm_model, test_data)
svm_prob_predictions <- predict(svm_model, test_data, type = "prob")[, "High"]
svm_conf_matrix <- confusionMatrix(svm_predictions, test_data$Infection_Rate_Class)
svm_roc_obj <- roc(test_data$Infection_Rate_Class, svm_prob_predictions)
cat("\nSVM Confusion Matrix and Metrics:\n")
print(svm_conf_matrix)
cat("\nSVM AUC-ROC:\n")
print(auc(svm_roc_obj))
cat("\nSVM CV Results:\n")
print(svm_model$results) 

# SVM Training Performance
svm_train_predictions <- predict(svm_model, train_data)
svm_train_conf_matrix <- confusionMatrix(svm_train_predictions, train_data$Infection_Rate_Class)
cat("\nSVM Training Confusion Matrix:\n")
print(svm_train_conf_matrix)
```

The Cross validation ROC is the highest of the three models, coming in at about 0.763.The ROC score (Receiver Operating Characteristic curve) is a representation of a classifier's performance across all possible classification thresholds. The classification thresholds are decision boundaries, or cutoff values, used to assign a predicted probability or score to one of the class labels (Low or High). 

The sensitivity is about 72%. This sensitivity value shows the proportion of low cases correctly identified and this value is higher than both random forest and logistic regression. 

The specificity is the amount of high cases correctly identified. This comes in at about 69%, which is comparable to random forest (66%) and slightly lower than logistic regression (70%).

We have moderate variablity across folds with standard deviations (ROCSD, SensSD, SpecSD) of 0.064, 0.105, and 0.1134. The ROCSD is similar to random forest (0.0656), but higher than logistic regression (0.047). SensSD and SpecSD are lower than logistic regression, which suggests more stable class specific performance despite the small fold size. 

There is poor predictive power on the test set, coming in at 52%. This is not better than random guessing. SVM generalizes the least effectively in this case. 

Having the best ROC score means that SVM is the best model to use if there are lots of non-linear relationships involved in the data.



Permutation importance measures how much a model's performance (Accuracy or AUC-ROC) decrease when a feature's values are randomly changed (permuted). Features that cause a larger drop in performance when permuted are considered more important features. 

```{r}
# Extract permutation importance using varImp()
svm_importance <- varImp(svm_model, scale = TRUE)

# Display the importance scores
cat("\nSVM Permutation Importance:\n")
print(svm_importance)

# Optionally, plot the feature importance
plot(svm_importance, main = "Permutation Importance for SVM (Radial Kernel)")

```

We see that median age is the most important features for the SVM model. 

Now lets apply the same models to death rate... 


```{r}
# Random Forest 
# Step 3: Train and Assess Random Forest (Death Rate)
set.seed(123)
rf_tuned_death <- train(
  Death_Rate_Class ~ median_age + median_income + total_pop,
  data = train_data %>% mutate(Death_Rate_Class = dataset$Death_Rate_Class[train_index]),
  method = "rf",
  trControl = ctrl,
  tuneGrid = expand.grid(mtry = c(1, 2, 3)),
  metric = "ROC",
  ntree = 500
)

# Extract best model
best_rf_death <- rf_tuned_death$finalModel

# Cross-Validation Performance
cat("\nRandom Forest (Death Rate) Cross-Validation Results:\n")
print(rf_tuned_death$results)

# Test Set Performance
test_data_death <- test_data %>% mutate(Death_Rate_Class = dataset$Death_Rate_Class[-train_index])
rf_death_preds <- predict(best_rf_death, test_data)
rf_death_probs <- predict(best_rf_death, test_data, type = "prob")[, "High"]
rf_death_conf <- confusionMatrix(rf_death_preds, test_data_death$Death_Rate_Class)
rf_death_roc <- roc(test_data_death$Death_Rate_Class, rf_death_probs)
cat("\nRandom Forest (Death Rate) Test Confusion Matrix:\n")
print(rf_death_conf)
cat("AUC-ROC:\n")
print(auc(rf_death_roc))

# Training Performance
train_data_death <- train_data %>% mutate(Death_Rate_Class = dataset$Death_Rate_Class[train_index])
rf_death_train_preds <- predict(best_rf_death, train_data)
rf_death_train_conf <- confusionMatrix(rf_death_train_preds, train_data_death$Death_Rate_Class)
cat("\nTraining Confusion Matrix (Death Rate):\n")
print(rf_death_train_conf)
```

```{r}
# Logistic Regression 
# Logistic Regression (Death Rate)
lr_model_death <- train(
  Death_Rate_Class ~ median_age + median_income + total_pop,
  data = train_data_death,
  method = "glm",
  family = "binomial",
  trControl = ctrl,
  metric = "ROC"
)
lr_death_preds <- predict(lr_model_death, test_data)
lr_death_probs <- predict(lr_model_death, test_data, type = "prob")[, "High"]
lr_death_conf <- confusionMatrix(lr_death_preds, test_data_death$Death_Rate_Class)
lr_death_roc <- roc(test_data_death$Death_Rate_Class, lr_death_probs)
cat("\nLogistic Regression (Death Rate) Confusion Matrix:\n")
print(lr_death_conf)
cat("AUC-ROC:\n")
print(auc(lr_death_roc))
cat("\nLogistic Regression CV Results:\n")
print(lr_model_death$results)

# Training Performance
lr_death_train_preds <- predict(lr_model_death, train_data)
lr_death_train_conf <- confusionMatrix(lr_death_train_preds, train_data_death$Death_Rate_Class)
cat("\nTraining Confusion Matrix (Death Rate):\n")
print(lr_death_train_conf)

```


```{r}
# SVM 
svm_model_death <- train(
  Death_Rate_Class ~ median_age + median_income + total_pop,
  data = train_data_death,
  method = "svmRadial",
  trControl = ctrl,
  tuneGrid = expand.grid(sigma = c(0.1, 0.5, 1), C = c(0.1, 1, 10)),
  metric = "ROC"
)
svm_death_preds <- predict(svm_model_death, test_data)
svm_death_probs <- predict(svm_model_death, test_data, type = "prob")[, "High"]
svm_death_conf <- confusionMatrix(svm_death_preds, test_data_death$Death_Rate_Class)
svm_death_roc <- roc(test_data_death$Death_Rate_Class, svm_death_probs)
cat("\nSVM (Death Rate) Confusion Matrix:\n")
print(svm_death_conf)
cat("AUC-ROC:\n")
print(auc(svm_death_roc))
cat("\nSVM CV Results:\n")
print(svm_model_death$results)

# Training Performance
svm_death_train_preds <- predict(svm_model_death, train_data)
svm_death_train_conf <- confusionMatrix(svm_death_train_preds, train_data_death$Death_Rate_Class)
cat("\nTraining Confusion Matrix (Death Rate):\n")
print(svm_death_train_conf)


```

### Section 3 -- Evaluation 

## Infection Rate Evaluation

Logistic Regression is the most useful for making strategic descions due to its interpretability. Johnson and Johnson can use the coefficients to understand demographic risk factors (e.g., focusing on counties with younger populations if median_age has a negative coefficient), tailoring interventions accordingly.

Random Forest provides some utility through feature importance, but its overfitting (training 1.0 vs. test 0.7143) reduces confidence in its predictions for strategic planning.

SVM is the least useful, as its poor test performance and lack of interpretability make it unsuitable for informing targeted interventions.

## Death Rate Evaluation 

....






 


