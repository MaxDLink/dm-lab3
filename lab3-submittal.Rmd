---
output:
  html_document: default
  pdf_document: default
---

# Data Mining Lab 3 - Classification

---
Title: Data Mining Lab 3
Authors: Max Link, Logan Lu, Jadon Klipsch
Date: "2025-04-14"
Description: In this project, we will focus on classification 
---

```{r setup, include=FALSE}
# Disable RStudio's package update check for this session
options("rstudio.package_manager.check_updates" = FALSE)

# Set the CRAN repository
options(repos = c(CRAN = "https://cloud.r-project.org"))

# Install packages quietly
if (!requireNamespace("xfun", quietly = TRUE)) {
  install.packages("xfun", quietly = TRUE)
}
if (!requireNamespace("ggrepel", quietly = TRUE)) {
  install.packages("ggrepel", quietly = TRUE)
}

# Set knitr options
knitr::opts_chunk$set(echo = TRUE)

# Load required libraries and suppress startup messages
suppressPackageStartupMessages({
  library(dplyr)      # For data manipulation
  library(ggplot2)    # For visualizations
  library(tidyr)      # For cleaning data
  library(ggrepel)    # For repelling labels in plots
})
```

```{r, results='hide', message=FALSE, warning=FALSE}
# add packages to install here
pkgs <- c("tidyverse", "factoextra", "cluster", "patchwork", "tibble", "ggrepel",
          "mclust", "mcclust", "fpc", "seriation", "apcluster", "dbscan", "entropy", "maps", "kernlab", "skimr", "caret", "randomForest", "pROC", "cli")
pkgs_install <- pkgs[!(pkgs %in% installed.packages()[,"Package"])]
if(length(pkgs_install)) install.packages(pkgs_install)

suppressPackageStartupMessages(library(tidyverse))
suppressPackageStartupMessages(library(readr))
suppressPackageStartupMessages(library(dplyr))
suppressPackageStartupMessages(library(tidyr))
suppressPackageStartupMessages(library(knitr))
suppressPackageStartupMessages(library(tibble))
suppressPackageStartupMessages(library(mclust))
suppressPackageStartupMessages(library(mcclust))
suppressPackageStartupMessages(library(ggplot2))
suppressPackageStartupMessages(library(ggrepel))
suppressPackageStartupMessages(library(cluster))
suppressPackageStartupMessages(library(patchwork))
suppressPackageStartupMessages(library(factoextra))
suppressPackageStartupMessages(library(fpc))
suppressPackageStartupMessages(library(seriation))
suppressPackageStartupMessages(library(apcluster))
suppressPackageStartupMessages(library(dbscan))
suppressPackageStartupMessages(library(entropy))
suppressPackageStartupMessages(library(maps))
suppressPackageStartupMessages(library(skimr))
suppressPackageStartupMessages(library(caret))
suppressPackageStartupMessages(library(randomForest))
suppressPackageStartupMessages(library(pROC))
suppressPackageStartupMessages(library(cli))

``` 

```{r, results='hide', message=FALSE, warning=FALSE}


# Load libraries
library(tidyverse)
library(readr)
library(dplyr)
library(tidyr)
library(knitr)
library(tibble)
library(mclust)
library(mcclust)
library(ggplot2)
library(ggrepel)
library(cluster)
library(patchwork)
library(factoextra)
library(fpc)
library(seriation)
library(apcluster)
library(dbscan)
library(entropy)
library(maps)
library(skimr)


```

```{r}

dataset <- read.csv("tx_data_with_gr.csv")

# print(dataset)
```

### Section 1 -- Data Prep

First, we need to define our classes.

We can pick out two classes: 

1) High vs Low Infection Rate 



2) High vs Low Death Rate 



Now we can explore statistics to get proper thresholds

```{r}
# 1) Explore distribution of infection_rate and death_rate
# Summary statistics
cat("Summary of Infection Rate:\n")
summary(dataset$infection_rate)
cat("\nSummary of Death Rate:\n")
summary(dataset$death_rate)

```
The median is appropriate for both of these classification tasks because choosing the median balances our classes. Dealing with balanced classes means that we have a close 50-50 county split into high and low categories. This even split enables logistic regression, decision trees, SVMs. 

The infection rate threshold should be 0.07212. 

The death rate threshold should be 0.022603. 

```{r}
# Load required library (optional, for table creation)
library(dplyr)

# Read the dataset (update path as needed)
dataset <- read.csv("tx_data_with_gr.csv", stringsAsFactors = FALSE)

# Total number of counties
total_counties <- nrow(dataset)

# 1) High vs. Low Infection Rate (threshold: 0.1)
# Count counties with infection_rate > 0.1 (High)
high_infection <- sum(dataset$infection_rate > 0.07212, na.rm = TRUE)
# Count counties with infection_rate <= 0.1 (Low)
low_infection <- sum(dataset$infection_rate <= 0.07212, na.rm = TRUE)
# Calculate percentages
high_infection_pct <- (high_infection / total_counties) * 100
low_infection_pct <- (low_infection / total_counties) * 100

# 2) High vs. Low Death Rate (threshold: 0.023, updated from 0.0023)
# Count counties with death_rate > 0.023 (High)
high_death <- sum(dataset$death_rate > 0.022603, na.rm = TRUE)
# Count counties with death_rate <= 0.023 (Low)
low_death <- sum(dataset$death_rate <= 0.022603, na.rm = TRUE)
# Calculate percentages
high_death_pct <- (high_death / total_counties) * 100
low_death_pct <- (low_death / total_counties) * 100

# Print results
cat("1) Infection Rate Classification (Threshold: 0.07212, i.e., 1,000 cases per 10,000 population)\n")
cat("High Infection Rate (> 0.07212):", high_infection, "counties (", round(high_infection_pct, 2), "%)\n")
cat("Low Infection Rate (<= 0.07212):", low_infection, "counties (", round(low_infection_pct, 2), "%)\n\n")

cat("2) Death Rate Classification (Threshold: 0.022603, i.e., 230 deaths per 10,000 population)\n")
cat("High Death Rate (> 0.022603):", high_death, "counties (", round(high_death_pct, 2), "%)\n")
cat("Low Death Rate (<= 0.022603):", low_death, "counties (", round(low_death_pct, 2), "%)\n\n")

```

Now we can make our table with our class attributes for infection rate classification and death rate classification. 

```{r}
# Create class attributes for Infection Rate and Death Rate
dataset <- dataset %>%
  mutate(Infection_Rate_Class = ifelse(infection_rate > 0.07212, "High", "Low"),
         Death_Rate_Class = ifelse(death_rate > 0.022603, "High", "Low"))

# Select features for model learning, including both class attributes
table_for_model <- dataset %>%
  select(county_name, median_age, median_income, total_pop, death_rate, infection_rate,
         Infection_Rate_Class, Death_Rate_Class)

# Display first 10 rows
cat("\nSingle Table with Infection_Rate_Class and Death_Rate_Class (First 10 Rows):\n")
print(head(table_for_model, 10))

# Verify class distributions
# Infection Rate
infection_dist <- table(dataset$Infection_Rate_Class)
infection_prop <- prop.table(infection_dist)
cat("\nInfection Rate Class Distribution:\n")
print(infection_dist)
cat("\nInfection Rate Class Proportions:\n")
print(infection_prop)

# Death Rate
death_dist <- table(dataset$Death_Rate_Class)
death_prop <- prop.table(death_dist)
cat("\nDeath Rate Class Distribution:\n")
print(death_dist)
cat("\nDeath Rate Class Proportions:\n")
print(death_prop)

# Save the table
write.csv(table_for_model, "tx_data_with_class.csv", row.names = FALSE)

```

Our predictive features are: 

1) median age 

2) median income 

3) total population 


### Section 2 -- Modeling 

```{r}
# Load required libraries
library(dplyr)
library(caret)
library(randomForest)
library(pROC)

```

```{r} 
# Dataset loading and verification 

# Check if required columns exist
if (!all(c("infection_rate", "death_rate", "median_age", "median_income", "total_pop") %in% names(dataset))) {
  stop("Required columns (infection_rate, death_rate, median_age, median_income, total_pop) not found in dataset.")
}

```

```{r}
# Step 2: Data Preparation
# Create class attributes and ensure they are factors
dataset <- dataset %>%
  mutate(
    Infection_Rate_Class = factor(ifelse(infection_rate > 0.07212, "High", "Low"), levels = c("Low", "High")),
    Death_Rate_Class = factor(ifelse(death_rate > 0.022603, "High", "Low"), levels = c("Low", "High"))
  )

# Check class distribution
cat("Class distribution for Infection_Rate_Class:\n")
print(table(dataset$Infection_Rate_Class))

# Select features and class attribute
model_data <- dataset %>%
  select(median_age, median_income, total_pop, Infection_Rate_Class)

# Check for missing values
cat("\nMissing Values:\n")
print(colSums(is.na(model_data)))

# Handle missing values (impute with median if any)
model_data <- model_data %>%
  mutate(
    median_age = ifelse(is.na(median_age), median(median_age, na.rm = TRUE), median_age),
    median_income = ifelse(is.na(median_income), median(median_income, na.rm = TRUE), median_income),
    total_pop = ifelse(is.na(total_pop), median(total_pop, na.rm = TRUE), total_pop)
  )

# Scale numerical features
model_data <- model_data %>%
  mutate(
    median_age = scale(median_age),
    median_income = scale(median_income),
    total_pop = scale(total_pop)
  )

# Train-test split (80% train, 20% test, stratified)
set.seed(123)
train_index <- createDataPartition(model_data$Infection_Rate_Class, p = 0.8, list = FALSE)
train_data <- model_data[train_index, ]
test_data <- model_data[-train_index, ]

# Verify class distribution in train and test sets
cat("Training Set Class Distribution:\n")
print(table(train_data$Infection_Rate_Class))
cat("Testing Set Class Distribution:\n")
print(table(test_data$Infection_Rate_Class))

```

We can use Random Forest as our model because we want to classify 213 counties, which is a small sample size. Random Forest is appropriate for its ability to handle non-linear relationships, and prevent over-fitting.


```{r}

# Step 3: Train and Assess Random Forest
# Define train Control for cross-validation
ctrl <- trainControl(
  method = "cv",
  number = 5, # number of cross validation folds 
  summaryFunction = twoClassSummary,
  classProbs = TRUE,
  savePredictions = TRUE
) 

# Train Random Forest with tuning
set.seed(123)
rf_tuned <- train(
  Infection_Rate_Class ~ median_age + median_income + total_pop,
  data = train_data,
  method = "rf",
  trControl = ctrl,
  tuneGrid = expand.grid(mtry = c(1, 2, 3)),
  metric = "ROC",
  ntree = 500 # use 500 trees 
)

# Extract best model
best_model <- rf_tuned$finalModel

# Cross-Validation Performance
cat("\nRandom Forest Cross-Validation Results:\n")
print(rf_tuned$results)

# Test Set Performance
rf_test_predictions <- predict(best_model, test_data)
rf_test_prob_predictions <- predict(best_model, test_data, type = "prob")[, "High"]
rf_test_conf_matrix <- confusionMatrix(rf_test_predictions, test_data$Infection_Rate_Class)
rf_test_roc_obj <- roc(test_data$Infection_Rate_Class, rf_test_prob_predictions)
cat("\nRandom Forest Test Set Confusion Matrix and Metrics:\n")
print(rf_test_conf_matrix)
cat("Random Forest Test Set AUC-ROC:\n")
print(auc(rf_test_roc_obj))

# Training Set Performance
rf_train_predictions <- predict(best_model, train_data)
rf_train_conf_matrix <- confusionMatrix(rf_train_predictions, train_data$Infection_Rate_Class)
cat("\nRandom Forest Training Set Confusion Matrix and Metrics:\n")
print(rf_train_conf_matrix)


```

Our Accuracy is significantly better than random guessing with 71%, so Random Forest does have predictive power. However, Random Forest misclassifies 6 low counties as high counties and 6 high counties as low counties, which is a 28.57% error rate per class (6/21). This is substantial for a small test set of only 42 samples. In a public health context, this could have severe consequences like misallocating resources. 

Looking at the training Accuracy, we see that Random Forest has classified the training set perfectly with an accuracy of 1. This means that Random Forest has fully memorized the training set. This makes sense because random forest has extreme flexibility and the data has a small feature set (only 3 features), however the accuracy drop off of 71% for the testing set suggests severe data overfitting. This demonstrates Random Forest's ability to fit data, but raises concerns about overfitting data. This suggests that Random Forest may be too complex for this dataset and that simpler models like linear regression may perform better. 

This is backed up by the Kappa, which reads only 42%. This means that the model is only moderately above classifying by chance. 

A 95% Confidence Interval (CI) is wide, which means there is uncertainty about the test set. 

The p-value of 1.0 means there is no significant difference between errors in test sets, reflecting the balanced misclassification of 6 counties per test set. 

We have cross validation performance and training set performance with a confusion matrix/metrics. Looking at the cross validation results, we see the Mtry values ranging from 1 to 3. Mtry specifies the number of features that are used to determine the best split at a node. We have three predictors, so Mtry can be 1 (median age), 2 (median age, median income), or 3 (median age, median income, total population).


Mtry equal to 2 has the highest ROC score. Having an ROC (ability to discriminate between high and low classes) around 0.67 is moderate. An ROC of 0.67 is moderate because it is below 0.70, which means that total population, median age, and median income are not strong predictors of infection rate. 

The sens (sensitivity) reading identifies the positive class, which in this case is low. About 55% is low which means the random forest model struggles to identify low infection rate counties. 

Specificity is the proportion of high cases correctly identified. About 66% is moderate, which means our random forest model misses many high case counties in its classification. 

The standard deviations (ROCSD, SensSD, SpecSD) show variability across Cross validation folds. mtry 2 has ROCD of 0.0656 which suggests moderate variability in performance. This is most likely from our small dataset of only 171 counties in our training set. 


Random Forest may not capture linear relationships as well as linear regression or interactions as well as SVM, so now we can try those. 

```{r}
# Model 2: Logistic Regression
lr_model <- train(
  Infection_Rate_Class ~ median_age + median_income + total_pop,
  data = train_data,
  method = "glm",
  family = "binomial",
  trControl = ctrl,
  metric = "ROC"
)
lr_predictions <- predict(lr_model, test_data)
lr_prob_predictions <- predict(lr_model, test_data, type = "prob")[, "High"]
lr_conf_matrix <- confusionMatrix(lr_predictions, test_data$Infection_Rate_Class)
lr_roc_obj <- roc(test_data$Infection_Rate_Class, lr_prob_predictions)
cat("\nLogistic Regression Confusion Matrix and Metrics:\n")
print(lr_conf_matrix)
cat("\nLogistic Regression AUC-ROC:\n")
print(auc(lr_roc_obj))
cat("\nLogistic Regression CV Results:\n")
print(lr_model$results)

# Logistic Regression Training Performance
lr_train_predictions <- predict(lr_model, train_data)
lr_train_conf_matrix <- confusionMatrix(lr_train_predictions, train_data$Infection_Rate_Class)
cat("\nLogistic Regression Training Confusion Matrix:\n")
print(lr_train_conf_matrix)

```

```{r}

# Model 3: SVM with Radial Kernel
svm_model <- train(
  Infection_Rate_Class ~ median_age + median_income + total_pop,
  data = train_data,
  method = "svmRadial",
  trControl = ctrl,
  tuneGrid = expand.grid(sigma = c(0.1, 0.5, 1), C = c(0.1, 1, 10)),
  metric = "ROC"
)
svm_predictions <- predict(svm_model, test_data)
svm_prob_predictions <- predict(svm_model, test_data, type = "prob")[, "High"]
svm_conf_matrix <- confusionMatrix(svm_predictions, test_data$Infection_Rate_Class)
svm_roc_obj <- roc(test_data$Infection_Rate_Class, svm_prob_predictions)
cat("\nSVM Confusion Matrix and Metrics:\n")
print(svm_conf_matrix)
cat("\nSVM AUC-ROC:\n")
print(auc(svm_roc_obj))
cat("\nSVM CV Results:\n")
print(svm_model$results) 

# SVM Training Performance
svm_train_predictions <- predict(svm_model, train_data)
svm_train_conf_matrix <- confusionMatrix(svm_train_predictions, train_data$Infection_Rate_Class)
cat("\nSVM Training Confusion Matrix:\n")
print(svm_train_conf_matrix)
```

```{r}
# Step 4: Compare Models
results <- data.frame(
  Model = c("Random Forest", "Logistic Regression", "SVM"),
  Accuracy = c(rf_conf_matrix$overall["Accuracy"], lr_conf_matrix$overall["Accuracy"], svm_conf_matrix$overall["Accuracy"]),
  AUC_ROC = c(auc(rf_roc_obj), auc(lr_roc_obj), auc(svm_roc_obj)),
  Sensitivity = c(rf_conf_matrix$byClass["Sensitivity"], lr_conf_matrix$byClass["Sensitivity"], svm_conf_matrix$byClass["Sensitivity"]),
  Specificity = c(rf_conf_matrix$byClass["Specificity"], lr_conf_matrix$byClass["Specificity"], svm_conf_matrix$byClass["Specificity"])
)
cat("\nModel Comparison:\n")
print(results)

```

